{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tGGiQfWLp7NY"
   },
   "source": [
    "# **Text generation with an RNN**\n",
    "This tutorial demonstrates how to generate text using a character-based RNN. We will work with a Shakespeare dataset. Given a sequence of characters from this data (\"Shakespear\"), training a model to predict the next character in the sequence. Longer sequences of text can be generated by calling the model repeatedly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9GoT7pePqfD9"
   },
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "UydERZ-AeVYq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ow6j6HtUqlYu"
   },
   "source": [
    "### Downloading the Shakespeare dataset and Exploring it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tGggj9apenou",
    "outputId": "75a63085-d9d0-4ee7-96b5-176735fc06c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_url = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
    "dataset_text = open(data_url, 'rb').read().decode(encoding='utf-8')\n",
    "print(dataset_text[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TlsWrKfDe8AL",
    "outputId": "25267e9b-6a98-476b-e4dd-bf053896c36e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1115394"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oc2TRaC3fH3R",
    "outputId": "915f08b0-3ba8-4e1c-87aa-5c22ca65aa3d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 unique characters\n"
     ]
    }
   ],
   "source": [
    "# obtain the unique characters in the dataset and print out their length \n",
    "vocab = sorted(set(dataset_text))\n",
    "print ('{} unique characters'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jde8EHvZfLka",
    "outputId": "00f4958a-fe6e-4394-f07e-a3b5de3ee509"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n',\n",
       " ' ',\n",
       " '!',\n",
       " '$',\n",
       " '&',\n",
       " \"'\",\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '3',\n",
       " ':',\n",
       " ';',\n",
       " '?',\n",
       " 'A',\n",
       " 'B',\n",
       " 'C',\n",
       " 'D',\n",
       " 'E',\n",
       " 'F',\n",
       " 'G',\n",
       " 'H',\n",
       " 'I',\n",
       " 'J',\n",
       " 'K',\n",
       " 'L',\n",
       " 'M',\n",
       " 'N',\n",
       " 'O',\n",
       " 'P',\n",
       " 'Q',\n",
       " 'R',\n",
       " 'S',\n",
       " 'T',\n",
       " 'U',\n",
       " 'V',\n",
       " 'W',\n",
       " 'X',\n",
       " 'Y',\n",
       " 'Z',\n",
       " 'a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'j',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'q',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'v',\n",
       " 'w',\n",
       " 'x',\n",
       " 'y',\n",
       " 'z']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O7RwUS2Nqycg"
   },
   "source": [
    "### Processing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "syMWm17mfPBN",
    "outputId": "5b0caa39-1767-43ef-b60c-cf2c97b58c37"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vocab' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\ELMOST~1\\AppData\\Local\\Temp/ipykernel_3136/3821965790.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Creating a mapping from unique characters to indices\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mchar2idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mchar\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mindex\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchar\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mchar2idx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'vocab' is not defined"
     ]
    }
   ],
   "source": [
    "# Creating a mapping from unique characters to indices\n",
    "char2idx = {char:index for index, char in enumerate(vocab)}\n",
    "char2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "riOGEpAxfYWa",
    "outputId": "201e0139-1c0e-4adf-8998-1bc5aa29b5e5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '\\n',\n",
       " 1: ' ',\n",
       " 2: '!',\n",
       " 3: '$',\n",
       " 4: '&',\n",
       " 5: \"'\",\n",
       " 6: ',',\n",
       " 7: '-',\n",
       " 8: '.',\n",
       " 9: '3',\n",
       " 10: ':',\n",
       " 11: ';',\n",
       " 12: '?',\n",
       " 13: 'A',\n",
       " 14: 'B',\n",
       " 15: 'C',\n",
       " 16: 'D',\n",
       " 17: 'E',\n",
       " 18: 'F',\n",
       " 19: 'G',\n",
       " 20: 'H',\n",
       " 21: 'I',\n",
       " 22: 'J',\n",
       " 23: 'K',\n",
       " 24: 'L',\n",
       " 25: 'M',\n",
       " 26: 'N',\n",
       " 27: 'O',\n",
       " 28: 'P',\n",
       " 29: 'Q',\n",
       " 30: 'R',\n",
       " 31: 'S',\n",
       " 32: 'T',\n",
       " 33: 'U',\n",
       " 34: 'V',\n",
       " 35: 'W',\n",
       " 36: 'X',\n",
       " 37: 'Y',\n",
       " 38: 'Z',\n",
       " 39: 'a',\n",
       " 40: 'b',\n",
       " 41: 'c',\n",
       " 42: 'd',\n",
       " 43: 'e',\n",
       " 44: 'f',\n",
       " 45: 'g',\n",
       " 46: 'h',\n",
       " 47: 'i',\n",
       " 48: 'j',\n",
       " 49: 'k',\n",
       " 50: 'l',\n",
       " 51: 'm',\n",
       " 52: 'n',\n",
       " 53: 'o',\n",
       " 54: 'p',\n",
       " 55: 'q',\n",
       " 56: 'r',\n",
       " 57: 's',\n",
       " 58: 't',\n",
       " 59: 'u',\n",
       " 60: 'v',\n",
       " 61: 'w',\n",
       " 62: 'x',\n",
       " 63: 'y',\n",
       " 64: 'z'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2char = {index:char for index, char in enumerate(vocab)}\n",
    "idx2char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fZ64SqOCflIg",
    "outputId": "c5d679f8-6b25-4ebb-cc11-cca9c6bdb750"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[18,\n",
       " 47,\n",
       " 56,\n",
       " 57,\n",
       " 58,\n",
       " 1,\n",
       " 15,\n",
       " 47,\n",
       " 58,\n",
       " 47,\n",
       " 64,\n",
       " 43,\n",
       " 52,\n",
       " 10,\n",
       " 0,\n",
       " 14,\n",
       " 43,\n",
       " 44,\n",
       " 53,\n",
       " 56,\n",
       " 43,\n",
       " 1,\n",
       " 61,\n",
       " 43,\n",
       " 1,\n",
       " 54,\n",
       " 56,\n",
       " 53,\n",
       " 41,\n",
       " 43,\n",
       " 43,\n",
       " 42,\n",
       " 1,\n",
       " 39,\n",
       " 52,\n",
       " 63,\n",
       " 1,\n",
       " 44,\n",
       " 59,\n",
       " 56,\n",
       " 58,\n",
       " 46,\n",
       " 43,\n",
       " 56,\n",
       " 6,\n",
       " 1,\n",
       " 46,\n",
       " 43,\n",
       " 39,\n",
       " 56,\n",
       " 1,\n",
       " 51,\n",
       " 43,\n",
       " 1,\n",
       " 57,\n",
       " 54,\n",
       " 43,\n",
       " 39,\n",
       " 49,\n",
       " 8,\n",
       " 0,\n",
       " 0,\n",
       " 13,\n",
       " 50,\n",
       " 50,\n",
       " 10,\n",
       " 0,\n",
       " 31,\n",
       " 54,\n",
       " 43,\n",
       " 39,\n",
       " 49,\n",
       " 6,\n",
       " 1,\n",
       " 57,\n",
       " 54,\n",
       " 43,\n",
       " 39,\n",
       " 49,\n",
       " 8,\n",
       " 0,\n",
       " 0,\n",
       " 18,\n",
       " 47,\n",
       " 56,\n",
       " 57,\n",
       " 58,\n",
       " 1,\n",
       " 15,\n",
       " 47,\n",
       " 58,\n",
       " 47,\n",
       " 64,\n",
       " 43,\n",
       " 52,\n",
       " 10,\n",
       " 0,\n",
       " 37,\n",
       " 53,\n",
       " 59,\n",
       " 1,\n",
       " 39,\n",
       " 56,\n",
       " 43,\n",
       " 1,\n",
       " 39,\n",
       " 50,\n",
       " 50,\n",
       " 1,\n",
       " 56,\n",
       " 43,\n",
       " 57,\n",
       " 53,\n",
       " 50,\n",
       " 60,\n",
       " 43,\n",
       " 42,\n",
       " 1,\n",
       " 56,\n",
       " 39,\n",
       " 58,\n",
       " 46,\n",
       " 43,\n",
       " 56,\n",
       " 1,\n",
       " 58,\n",
       " 53,\n",
       " 1,\n",
       " 42,\n",
       " 47,\n",
       " 43,\n",
       " 1,\n",
       " 58,\n",
       " 46,\n",
       " 39,\n",
       " 52,\n",
       " 1,\n",
       " 58,\n",
       " 53,\n",
       " 1,\n",
       " 44,\n",
       " 39,\n",
       " 51,\n",
       " 47,\n",
       " 57,\n",
       " 46,\n",
       " 12,\n",
       " 0,\n",
       " 0,\n",
       " 13,\n",
       " 50,\n",
       " 50,\n",
       " 10,\n",
       " 0,\n",
       " 30,\n",
       " 43,\n",
       " 57,\n",
       " 53,\n",
       " 50,\n",
       " 60,\n",
       " 43,\n",
       " 42,\n",
       " 8,\n",
       " 1,\n",
       " 56,\n",
       " 43,\n",
       " 57,\n",
       " 53,\n",
       " 50,\n",
       " 60,\n",
       " 43,\n",
       " 42,\n",
       " 8,\n",
       " 0,\n",
       " 0,\n",
       " 18,\n",
       " 47,\n",
       " 56,\n",
       " 57,\n",
       " 58,\n",
       " 1,\n",
       " 15,\n",
       " 47,\n",
       " 58,\n",
       " 47,\n",
       " 64,\n",
       " 43,\n",
       " 52,\n",
       " 10,\n",
       " 0,\n",
       " 18,\n",
       " 47,\n",
       " 56,\n",
       " 57,\n",
       " 58,\n",
       " 6,\n",
       " 1,\n",
       " 63,\n",
       " 53,\n",
       " 59,\n",
       " 1,\n",
       " 49,\n",
       " 52,\n",
       " 53,\n",
       " 61,\n",
       " 1,\n",
       " 15,\n",
       " 39,\n",
       " 47,\n",
       " 59,\n",
       " 57,\n",
       " 1,\n",
       " 25,\n",
       " 39,\n",
       " 56,\n",
       " 41,\n",
       " 47,\n",
       " 59,\n",
       " 57,\n",
       " 1,\n",
       " 47,\n",
       " 57,\n",
       " 1,\n",
       " 41,\n",
       " 46,\n",
       " 47,\n",
       " 43,\n",
       " 44,\n",
       " 1,\n",
       " 43,\n",
       " 52,\n",
       " 43,\n",
       " 51,\n",
       " 63,\n",
       " 1,\n",
       " 58,\n",
       " 53,\n",
       " 1,\n",
       " 58,\n",
       " 46,\n",
       " 43,\n",
       " 1,\n",
       " 54,\n",
       " 43,\n",
       " 53,\n",
       " 54,\n",
       " 50,\n",
       " 43,\n",
       " 8,\n",
       " 0]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the dataset from 'characters' to 'integers'\n",
    "text_as_int = [char2idx[char] for char in dataset_text]\n",
    "text_as_int[:250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1115394"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_as_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "CHF_khodhU-1"
   },
   "outputs": [],
   "source": [
    "# converting the text vector into a stream of character indices using from_tensor_slices function from tf.data.dataset\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c2G1sb-_hlLM",
    "outputId": "afde80f0-ba64-4f53-bbf8-1f4346021eea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F\n",
      "i\n",
      "r\n",
      "s\n",
      "t\n",
      " \n",
      "C\n",
      "i\n",
      "t\n",
      "i\n",
      "z\n",
      "e\n",
      "n\n",
      ":\n",
      "\n",
      "\n",
      "B\n",
      "e\n",
      "f\n",
      "o\n",
      "r\n",
      "e\n",
      " \n",
      "w\n",
      "e\n",
      " \n",
      "p\n",
      "r\n",
      "o\n",
      "c\n",
      "e\n",
      "e\n",
      "d\n",
      " \n",
      "a\n",
      "n\n",
      "y\n",
      " \n",
      "f\n",
      "u\n",
      "r\n",
      "t\n",
      "h\n",
      "e\n",
      "r\n",
      ",\n",
      " \n",
      "h\n",
      "e\n",
      "a\n",
      "r\n",
      " \n",
      "m\n",
      "e\n",
      " \n",
      "s\n",
      "p\n",
      "e\n",
      "a\n",
      "k\n",
      ".\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "A\n",
      "l\n",
      "l\n",
      ":\n",
      "\n",
      "\n",
      "S\n",
      "p\n",
      "e\n",
      "a\n",
      "k\n",
      ",\n",
      " \n",
      "s\n",
      "p\n",
      "e\n",
      "a\n",
      "k\n",
      ".\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "F\n",
      "i\n",
      "r\n",
      "s\n",
      "t\n",
      " \n",
      "C\n",
      "i\n",
      "t\n",
      "i\n",
      "z\n",
      "e\n",
      "n\n",
      ":\n",
      "\n",
      "\n",
      "Y\n",
      "o\n",
      "u\n",
      " \n",
      "a\n",
      "r\n",
      "e\n",
      " \n",
      "a\n",
      "l\n",
      "l\n",
      " \n",
      "r\n",
      "e\n",
      "s\n",
      "o\n",
      "l\n",
      "v\n",
      "e\n",
      "d\n",
      " \n",
      "r\n",
      "a\n",
      "t\n",
      "h\n",
      "e\n",
      "r\n",
      " \n",
      "t\n",
      "o\n",
      " \n",
      "d\n",
      "i\n",
      "e\n",
      " \n",
      "t\n",
      "h\n",
      "a\n",
      "n\n",
      " \n",
      "t\n",
      "o\n",
      " \n",
      "f\n",
      "a\n",
      "m\n",
      "i\n",
      "s\n",
      "h\n",
      "?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "A\n",
      "l\n",
      "l\n",
      ":\n",
      "\n",
      "\n",
      "R\n",
      "e\n",
      "s\n",
      "o\n",
      "l\n",
      "v\n",
      "e\n",
      "d\n",
      ".\n",
      " \n",
      "r\n",
      "e\n",
      "s\n",
      "o\n",
      "l\n",
      "v\n",
      "e\n",
      "d\n",
      ".\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "F\n",
      "i\n",
      "r\n",
      "s\n",
      "t\n",
      " \n",
      "C\n",
      "i\n",
      "t\n",
      "i\n",
      "z\n",
      "e\n",
      "n\n",
      ":\n",
      "\n",
      "\n",
      "F\n",
      "i\n",
      "r\n",
      "s\n",
      "t\n",
      ",\n",
      " \n",
      "y\n",
      "o\n",
      "u\n",
      " \n",
      "k\n",
      "n\n",
      "o\n",
      "w\n",
      " \n",
      "C\n",
      "a\n",
      "i\n",
      "u\n",
      "s\n",
      " \n",
      "M\n",
      "a\n",
      "r\n",
      "c\n",
      "i\n",
      "u\n",
      "s\n",
      " \n",
      "i\n",
      "s\n",
      " \n",
      "c\n",
      "h\n",
      "i\n",
      "e\n",
      "f\n",
      " \n",
      "e\n",
      "n\n",
      "e\n",
      "m\n",
      "y\n",
      " \n",
      "t\n",
      "o\n",
      " \n",
      "t\n",
      "h\n",
      "e\n",
      " \n",
      "p\n",
      "e\n",
      "o\n",
      "p\n",
      "l\n",
      "e\n",
      ".\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# visualizing some chars from char_dataset\n",
    "for i in char_dataset.take(250):\n",
    "  print(idx2char[i.numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tmd_y75emkwV"
   },
   "outputs": [],
   "source": [
    "# function to convert ids to text\n",
    "def idx2text(ids):\n",
    "  return ''.join([idx2char[i] for i in ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qWdr_jYjh_lG",
    "outputId": "4fcf74f3-4d6c-402c-9346-686006d675bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You \n",
      "are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you k\n",
      "now Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us ki\n",
      "ll him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be d\n",
      "one: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citi\n"
     ]
    }
   ],
   "source": [
    "# dividing the text into example sequences. Each input sequence will contain seq_length characters from the text.\n",
    "seq_length = 100\n",
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "for item in sequences.take(5):\n",
    "  print(idx2text(item.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "apjhf8_uiL9t"
   },
   "outputs": [],
   "source": [
    "# For each sequence, we duplicated and shifted it to form the input and target text by using the `map` method to apply a simple function to each batch:\n",
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ljjkw5Asj4qw",
    "outputId": "31e15ce8-b384-45a2-da68-74d9f9692e60",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data:\n",
      " First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n",
      "---------------------------------------------------------------------\n",
      "Target data:\n",
      " irst Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You \n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in  dataset.take(1):\n",
    "  print ('Input data:\\n',idx2text(input_example.numpy()))\n",
    "  print(\"---------------------------------------------------------------------\")\n",
    "  print ('Target data:\\n',idx2text(target_example.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I8PlgcgVkIVd"
   },
   "outputs": [],
   "source": [
    "# Shuffling the dataset and it into batches\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JTjSj7H0urlD"
   },
   "source": [
    "### Building and training the model\n",
    "Using tf.keras.Sequential to define the model. Three layers are used:\n",
    "\n",
    "- tf.keras.layers.Embedding: The first layer that mapping the numbers of each character to a vector with embedding_dim dimensions\n",
    "- tf.keras.layers.GRU\n",
    "- tf.keras.layers.Dense: The output layer, with vocab_size outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FBsF5ZxcvhAQ"
   },
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qXzf5GB0WeNP"
   },
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "  model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim,batch_input_shape=[batch_size, None]),\n",
    "    tf.keras.layers.GRU(rnn_units,return_sequences=True,stateful=True,recurrent_initializer='glorot_uniform'),\n",
    "    tf.keras.layers.Dense(vocab_size)\n",
    "  ])\n",
    "  return model\n",
    "\n",
    "model = build_model(vocab_size = len(vocab),embedding_dim=embedding_dim,rnn_units=rnn_units,batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MR-R6ncA9AeI",
    "outputId": "56ee37c9-b660-41c4-f760-cff48cfa47ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (64, None, 256)           16640     \n",
      "                                                                 \n",
      " gru (GRU)                   (64, None, 1024)          3938304   \n",
      "                                                                 \n",
      " dense (Dense)               (64, None, 65)            66625     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,021,569\n",
      "Trainable params: 4,021,569\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UOrJ4UrSWeHv",
    "outputId": "a87e518a-937e-4935-a6ee-da5dde21ae76"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 65) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "  example_batch_predictions = model(input_example_batch)\n",
    "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "okHDU9rcvwMg"
   },
   "source": [
    "To get actual predictions from the model we need to sample from the output distribution, to get actual character indices. This distribution is defined by the logits over the character vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mKnKgxHhWd-k",
    "outputId": "126f886e-fdf8-4ce6-f384-ddc7f6d6202a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4, 45, 11, 40, 15,  5,  6, 16, 46, 19,  6, 40,  1, 11, 20, 58,  7,\n",
       "       18, 28, 30, 54, 20, 19, 47, 12, 21, 11, 16, 20, 15,  9, 59, 11, 37,\n",
       "        4, 48, 64, 47, 60, 40,  0, 62, 13, 40, 18, 37, 35,  7, 59, 44, 47,\n",
       "       34, 19, 41,  8, 59, 60, 22, 31, 55, 28, 46,  2,  0, 48, 23, 33, 53,\n",
       "       40, 58, 32, 53, 33, 41, 22, 46, 34, 39, 54, 26, 38, 30, 53, 48, 40,\n",
       "       28,  0, 58, 50, 53,  3, 43,  6,  4, 53, 20, 17, 19, 56,  2])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()\n",
    "sampled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OXSv-Nk3v73C",
    "outputId": "a7c745e3-ce1d-4c21-d629-09c8e2c9db8a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: \n",
      " of whereof, there is my honour's pawn;\n",
      "Engage it to the trial, if thou darest.\n",
      "\n",
      "LORD FITZWATER:\n",
      "How \n",
      "\n",
      "Next Char Predictions: \n",
      " &g;bC',DhG,b ;Ht-FPRpHGi?I;DHC3u;Y&jzivb\n",
      "xAbFYW-ufiVGc.uvJSqPh!\n",
      "jKUobtToUcJhVapNZRojbP\n",
      "tlo$e,&oHEGr!\n"
     ]
    }
   ],
   "source": [
    "# Results from an untrained model \n",
    "print(\"Input: \\n\", idx2text(input_example_batch[0].numpy()))\n",
    "print()\n",
    "print(\"Next Char Predictions: \\n\", idx2text(sampled_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AFr2ARxlZxuB",
    "outputId": "293f21fc-418a-481a-b410-f5fba18b46f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (64, 100, 65)  # (batch_size, sequence_length, vocab_size)\n",
      "scalar_loss:       4.1748095\n"
     ]
    }
   ],
   "source": [
    "# defining the loss and calculating it before training\n",
    "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-fZ91e3HZxq4"
   },
   "outputs": [],
   "source": [
    "# compiling the model\n",
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ij6N6MWdZxkd",
    "outputId": "9f28a3e1-0de2-49e8-d813-5d3b0cfad923"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "172/172 [==============================] - 20s 59ms/step - loss: 2.6674\n",
      "Epoch 2/20\n",
      "172/172 [==============================] - 11s 53ms/step - loss: 1.9585\n",
      "Epoch 3/20\n",
      "172/172 [==============================] - 11s 53ms/step - loss: 1.6903\n",
      "Epoch 4/20\n",
      "172/172 [==============================] - 11s 54ms/step - loss: 1.5438\n",
      "Epoch 5/20\n",
      "172/172 [==============================] - 11s 56ms/step - loss: 1.4555\n",
      "Epoch 6/20\n",
      "172/172 [==============================] - 11s 55ms/step - loss: 1.3964\n",
      "Epoch 7/20\n",
      "172/172 [==============================] - 11s 55ms/step - loss: 1.3503\n",
      "Epoch 8/20\n",
      "172/172 [==============================] - 11s 56ms/step - loss: 1.3127\n",
      "Epoch 9/20\n",
      "172/172 [==============================] - 12s 56ms/step - loss: 1.2791\n",
      "Epoch 10/20\n",
      "172/172 [==============================] - 12s 57ms/step - loss: 1.2466\n",
      "Epoch 11/20\n",
      "172/172 [==============================] - 11s 55ms/step - loss: 1.2141\n",
      "Epoch 12/20\n",
      "172/172 [==============================] - 11s 57ms/step - loss: 1.1828\n",
      "Epoch 13/20\n",
      "172/172 [==============================] - 12s 57ms/step - loss: 1.1513\n",
      "Epoch 14/20\n",
      "172/172 [==============================] - 12s 57ms/step - loss: 1.1174\n",
      "Epoch 15/20\n",
      "172/172 [==============================] - 11s 57ms/step - loss: 1.0845\n",
      "Epoch 16/20\n",
      "172/172 [==============================] - 12s 56ms/step - loss: 1.0488\n",
      "Epoch 17/20\n",
      "172/172 [==============================] - 11s 56ms/step - loss: 1.0136\n",
      "Epoch 18/20\n",
      "172/172 [==============================] - 11s 56ms/step - loss: 0.9793\n",
      "Epoch 19/20\n",
      "172/172 [==============================] - 11s 57ms/step - loss: 0.9448\n",
      "Epoch 20/20\n",
      "172/172 [==============================] - 11s 57ms/step - loss: 0.9112\n"
     ]
    }
   ],
   "source": [
    "# training the model\n",
    "history = model.fit(dataset, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZX4fckZjpB61"
   },
   "outputs": [],
   "source": [
    "# saving the model weights of the last epoch\n",
    "model.save_weights('model_weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ikKtS7sg9ay1"
   },
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vsqJsrjodwCT"
   },
   "outputs": [],
   "source": [
    "# building the model again with batch_size of 1 for prediction\n",
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "model.load_weights('/content/model_weights.h5')\n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0wBDU8pbdy10",
    "outputId": "6989d0da-a002-405f-c149-9b00871f0e70"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (1, None, 256)            16640     \n",
      "                                                                 \n",
      " gru_1 (GRU)                 (1, None, 1024)           3938304   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (1, None, 65)             66625     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,021,569\n",
      "Trainable params: 4,021,569\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YRc0gJbC9wr3"
   },
   "source": [
    "#### The prediction loop\n",
    "The following code block generates the text:\n",
    "\n",
    "- It Starts by choosing a start string, initializing the RNN state and setting the number of characters to generate.\n",
    "\n",
    "- Get the prediction distribution of the next character using the start string and the RNN state.\n",
    "\n",
    "- Then, use a categorical distribution to calculate the index of the predicted character. Use this predicted character as our next input to the model.\n",
    "\n",
    "- The RNN state returned by the model is fed back into the model so that it now has more context, instead than only one word. After predicting the next word, the modified RNN states are again fed back into the model, which is how it learns as it gets more context from the previously predicted words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EuGVknnSnVol"
   },
   "outputs": [],
   "source": [
    "\n",
    "def generate_text(model, start_string):\n",
    "  # Evaluation step (generating text using the learned model)\n",
    "\n",
    "  # Number of characters to generate\n",
    "  num_generate = 1000\n",
    "\n",
    "  # Converting our start string to numbers (vectorizing)\n",
    "  input_eval = [char2idx[s] for s in start_string]\n",
    "  input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "  # Empty string to store our results\n",
    "  text_generated = []\n",
    "\n",
    "  # Low temperatures results in more predictable text.\n",
    "  # Higher temperatures results in more surprising text.\n",
    "  # Experiment to find the best setting.\n",
    "  temperature = 1.0\n",
    "\n",
    "  # Here batch size == 1\n",
    "  model.reset_states()\n",
    "  for i in range(num_generate):\n",
    "      predictions = model(input_eval)\n",
    "      # remove the batch dimension\n",
    "      predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "      # using a categorical distribution to predict the word returned by the model\n",
    "      predictions = predictions / temperature\n",
    "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "      # We pass the predicted word as the next input to the model\n",
    "      # along with the previous hidden state\n",
    "      input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "      text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "  return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JzCa7VS-pTWL",
    "outputId": "e280c9a6-7d2f-46ed-c604-8a005cbfd501"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO: Grumio, go you twenty princely general: for this time,\n",
      "The golden carses for grise and my bring his lady passing breath: the\n",
      "seven'd with our trobberous death, is dust.\n",
      "Our ships mistaking to a second cause?\n",
      "\n",
      "KING RICHARD II:\n",
      "First, thenefore castle my for getting at your face?\n",
      "Prepare you, Catesby.\n",
      "Thou'rt any caust. This is another,\n",
      "Thou know'st, as now but one, that Henry, for this dire,\n",
      "And in their gartes shall be come.'\n",
      "\n",
      "PETRUCHIO:\n",
      "A deceit brooks, and sullen show\n",
      "our dustiness spoken:\n",
      "In all pleased for a little breeling left by the\n",
      "butt thou wast barrant forbid, come again,\n",
      "And I'll swear to rsy titles of thy soul!\n",
      "Councilst Richard, now methld not be brief.\n",
      "\n",
      "BUCKINGHAM:\n",
      "Why should be cold, All to pieces: boy, here's none;\n",
      "Away with her, she could be executine, so ling is done.\n",
      "The Volick, word if about\n",
      "To old freedom, rage: let me see thee better\n",
      "Thus doing the instrument delivers' heirs,\n",
      "A mark thee chamber-with world!\n",
      "Fear not, my ass me Swoon;'\n",
      "And, those Claudio to my wife\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, start_string=u\"ROMEO: \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SJrE3xZxpWPo"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
